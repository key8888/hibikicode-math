# 第7章 三次関数
## 7.8 誤差が最小になる直線とは？

### 説明＆コード
「一番データに近い直線」とは、誤差が一番小さい直線のこと。誤差の合計を小さくする考え方は“最適化”の入り口。ここでは簡単に、誤差の絶対値合計を計算して比較するよ。

```python
import numpy as np

data_x = np.array([0, 1, 2, 3, 4, 5, 6])
data_y = np.array([0.5, 1.0, 1.8, 2.7, 3.5, 4.2, 6.0])

candidates = [
    (0.7, 0.5),
    (0.9, 0.2),
    (1.1, -0.1)
]

for a, b in candidates:
    preds = a * data_x + b
    errors = data_y - preds
    total_error = np.sum(np.abs(errors))
    print(f"傾き {a}, 切片 {b} の誤差合計: {total_error:.2f}")
```

出力を見て、どの組み合わせが一番誤差の合計が小さいかチェックしよう。これが“最適”に近い直線になる。

#### ミニ課題
候補を3つ追加して、誤差合計が最小のものを探そう。どんな傾き・切片が良かった？

### 質問タイム
1. 誤差合計が小さい直線のほうが「フィットしている」と言える理由は？
2. 絶対値ではなく二乗の合計を使うと何が変わる？

### 振り返り
- [ ] 誤差合計を計算して比較する方法を理解できた？
- [ ] 最適化という言葉が、誤差を小さくすることとつながると感じた？

### 発展
二乗の合計を最小にする計算（最小二乗法）は、二次関数の頂点を求める話とつながる。次のステップで二次関数がどう効くか想像してみよう。
